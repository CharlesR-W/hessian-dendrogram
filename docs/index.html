<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Hessian Dendrograms &mdash; Tracking Circuit Formation Through Spectral Analysis</title>
<style>
  :root {
    --bg: #0f1117;
    --surface: #1a1d27;
    --border: #2a2d3a;
    --text: #e2e8f0;
    --text-muted: #94a3b8;
    --accent: #7dd3fc;
    --accent2: #6ee7b7;
    --warn: #fcd34d;
    --neg: #fca5a5;
    --purple: #c4b5fd;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    font-family: 'Georgia', 'Times New Roman', serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.7;
    max-width: 52rem;
    margin: 0 auto;
    padding: 2rem 1.5rem 4rem;
  }
  h1 {
    font-size: 1.9rem;
    margin-bottom: 0.3rem;
    color: var(--accent);
    letter-spacing: -0.02em;
  }
  h2 {
    font-size: 1.4rem;
    margin-top: 2.5rem;
    margin-bottom: 0.8rem;
    color: var(--accent2);
    border-bottom: 1px solid var(--border);
    padding-bottom: 0.3rem;
  }
  h3 {
    font-size: 1.15rem;
    margin-top: 1.5rem;
    margin-bottom: 0.5rem;
    color: var(--purple);
  }
  p { margin-bottom: 1rem; }
  a { color: var(--accent); text-decoration: none; }
  a:hover { text-decoration: underline; }
  .subtitle {
    color: var(--text-muted);
    font-size: 1.05rem;
    margin-bottom: 2rem;
  }
  .figure {
    margin: 1.5rem 0;
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    overflow: hidden;
  }
  .figure img {
    width: 100%;
    display: block;
  }
  .figure figcaption {
    padding: 0.7rem 1rem;
    font-size: 0.9rem;
    color: var(--text-muted);
    border-top: 1px solid var(--border);
    line-height: 1.5;
  }
  .callout {
    background: var(--surface);
    border-left: 3px solid var(--warn);
    padding: 1rem 1.2rem;
    margin: 1.2rem 0;
    border-radius: 0 6px 6px 0;
    font-size: 0.95rem;
  }
  .callout-neg {
    border-left-color: var(--neg);
  }
  .callout strong {
    color: var(--warn);
  }
  .callout-neg strong {
    color: var(--neg);
  }
  code {
    font-family: 'Fira Code', 'Consolas', monospace;
    background: var(--surface);
    padding: 0.15em 0.4em;
    border-radius: 3px;
    font-size: 0.88em;
  }
  .katex-like {
    font-style: italic;
    color: var(--accent);
  }
  ul, ol {
    margin-bottom: 1rem;
    padding-left: 1.5rem;
  }
  li { margin-bottom: 0.4rem; }
  .experiment-badge {
    display: inline-block;
    padding: 0.15em 0.6em;
    border-radius: 4px;
    font-size: 0.8rem;
    font-family: sans-serif;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.05em;
  }
  .badge-complete { background: #065f46; color: #6ee7b7; }
  .badge-wip { background: #713f12; color: #fcd34d; }
  hr {
    border: none;
    border-top: 1px solid var(--border);
    margin: 2.5rem 0;
  }
  .footer {
    color: var(--text-muted);
    font-size: 0.85rem;
    margin-top: 3rem;
    text-align: center;
  }
  .toc {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 1rem 1.5rem;
    margin: 1.5rem 0 2rem;
  }
  .toc ul { list-style: none; padding-left: 0; margin-bottom: 0; }
  .toc li { margin-bottom: 0.3rem; }
  .toc a { color: var(--text-muted); }
  .toc a:hover { color: var(--accent); }
  .grid-2 {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 1rem;
  }
  @media (max-width: 600px) {
    .grid-2 { grid-template-columns: 1fr; }
    body { padding: 1rem; }
  }
</style>
</head>
<body>

<h1>Hessian Dendrograms</h1>
<p class="subtitle">
  Tracking circuit formation in neural networks through Hessian eigenspectrum analysis
</p>

<nav class="toc">
  <ul>
    <li><a href="#motivation">1. Motivation: Circuits and Curvature</a></li>
    <li><a href="#measurement">2. What We Measure</a></li>
    <li><a href="#plots">3. Reading the Plots</a></li>
    <li><a href="#expected">4. What We Expected</a></li>
    <li><a href="#observed">5. What We Actually See</a></li>
    <li><a href="#discussion">6. Why the Mismatch?</a></li>
    <li><a href="#experiments">7. Experiments</a></li>
    <li><a href="#next">8. Next Steps</a></li>
  </ul>
</nav>


<h2 id="motivation">1. Motivation: Circuits and Curvature</h2>

<p>
Neural networks learn by developing internal computational <em>circuits</em> &mdash;
groups of weights that work together to implement a specific function
(edge detection, frequency filtering, modular arithmetic, etc.).  A
central question in mechanistic interpretability is: <strong>when and how do
these circuits form during training?</strong>
</p>

<p>
The <strong>Hessian matrix</strong> of the loss function &mdash; the matrix of all
second derivatives with respect to parameters &mdash; captures the local
<em>curvature structure</em> of the loss landscape.  Each eigenvalue of the
Hessian corresponds to a direction in parameter space with a specific
curvature: large eigenvalues indicate directions the loss is very
sensitive to, near-zero eigenvalues indicate flat directions the model
can wander along freely, and negative eigenvalues indicate directions
where the model sits at a saddle point.
</p>

<p>
The core hypothesis we're testing:
</p>

<div class="callout">
  <strong>Spectral circuit hypothesis:</strong>
  If a network implements <em>K</em> distinct circuits that are geometrically
  separable in parameter space, the Hessian eigenspectrum should exhibit
  <em>K</em> clusters of eigenvalues separated by spectral gaps.  Each cluster
  corresponds to a set of parameter directions that are coupled within a
  circuit (similar curvature) but decoupled from other circuits (separated
  by a gap).  As circuits form during training, these gaps should appear and
  sharpen.
</div>

<p>
This hypothesis connects to a broader idea: that circuit formation is a
<em>symmetry-breaking</em> process.  At initialization, the parameter space
looks roughly isotropic &mdash; all directions are similar.  As the network
learns, this symmetry breaks: some directions become critically important
(large eigenvalues), while others become irrelevant (near-zero eigenvalues).
The pattern of this breaking &mdash; how many groups form, at what scales, in
what order &mdash; could tell us about the computational structure the network
is building.
</p>


<h2 id="measurement">2. What We Measure</h2>

<p>
At checkpoints throughout training, we compute the Hessian eigenspectrum
and organize it hierarchically:
</p>

<ol>
  <li><strong>Compute eigenvalues.</strong>  For small models (&lt;10K params), we form
  the full Hessian and diagonalize it exactly.  For larger models, we
  use Lanczos iteration with Hessian-vector products to approximate the
  top and bottom eigenvalues without ever forming the full matrix.</li>

  <li><strong>Build a dendrogram.</strong>  We perform single-linkage hierarchical
  clustering on the sorted eigenvalue array.  At resolution
  <span class="katex-like">&epsilon;</span>, eigenvalues within
  <span class="katex-like">&epsilon;</span> of each other are merged into
  the same cluster.  As <span class="katex-like">&epsilon;</span> increases,
  clusters merge &mdash; producing a dendrogram whose branch heights
  correspond to spectral gaps.</li>

  <li><strong>Track evolution.</strong>  By comparing dendrograms across training
  checkpoints, we see how the hierarchical spectral structure changes
  as the network learns.</li>
</ol>

<p>
The key derived quantity is
<strong><span class="katex-like">n</span>(<span class="katex-like">&epsilon;</span>)</strong>:
the number of eigenvalue clusters at resolution
<span class="katex-like">&epsilon;</span>.  If the spectrum has clean band
structure, <span class="katex-like">n</span>(<span class="katex-like">&epsilon;</span>)
is a staircase &mdash; it drops sharply at each spectral gap.  If the spectrum
is continuous, <span class="katex-like">n</span>(<span class="katex-like">&epsilon;</span>)
decreases smoothly.
</p>


<h2 id="plots">3. Reading the Plots</h2>

<h3>Spectrum Evolution</h3>
<figure class="figure">
  <img src="figures/spectrum_evolution.png"
       alt="Eigenvalue spectrum evolution over training" />
  <figcaption>
    <strong>Positive eigenvalues (log scale) at each checkpoint.</strong>
    Each curve is one training snapshot.  At initialization (faint),
    eigenvalues are broadly distributed.  As training progresses (bright),
    the bulk concentrates near zero while a few outlier eigenvalues grow
    large &mdash; these are the directions the loss landscape cares most about.
    The shape of this curve is the raw data underlying everything else.
  </figcaption>
</figure>

<h3>Dendrogram Snapshots</h3>
<figure class="figure">
  <img src="figures/dendrogram_snapshots.png"
       alt="Dendrogram snapshots at selected checkpoints" />
  <figcaption>
    <strong>Truncated dendrograms (top 30 merges) at selected training stages.</strong>
    Each leaf represents a cluster of eigenvalues; the number in parentheses
    is the cluster size.  Branch height = the spectral gap that must be
    bridged to merge two clusters.  Tall branches mean large gaps in the
    spectrum.  If circuits existed as spectral bands, we'd see multiple
    tall branches of similar height, each separating a circuit.
  </figcaption>
</figure>

<h3>Cluster Count Heatmap</h3>
<figure class="figure">
  <img src="figures/cluster_heatmap.png"
       alt="Cluster count n(epsilon) heatmap over training" />
  <figcaption>
    <strong>Number of eigenvalue clusters as a function of resolution
    <span class="katex-like">&epsilon;</span> (vertical) and training step (horizontal).</strong>
    This is the 2D "fingerprint" of spectral structure.  Sharp horizontal
    boundaries (abrupt color changes at a fixed <span class="katex-like">&epsilon;</span>)
    indicate spectral gaps: resolutions where the cluster count drops suddenly.
    If the spectrum had clean band structure, you'd see distinct horizontal
    bands of constant color.  Smooth gradients indicate a continuous spectrum.
  </figcaption>
</figure>

<h3>Gap Barcode</h3>
<figure class="figure">
  <img src="figures/gap_barcode.png"
       alt="Top spectral gaps over training" />
  <figcaption>
    <strong>The top-<em>k</em> largest spectral gaps at each checkpoint.</strong>
    Each line tracks one gap (1st-largest, 2nd-largest, etc.) over training.
    If a circuit forms at step <em>t</em>, a new large gap should appear.
    The gap between the largest eigenvalue and the rest dominates here,
    with other gaps much smaller.
  </figcaption>
</figure>

<h3>Summary Statistics</h3>
<figure class="figure">
  <img src="figures/summary_stats.png"
       alt="Summary statistics over training" />
  <figcaption>
    <strong>Training metrics alongside spectral properties.</strong>
    <em>Trace</em> (sum of eigenvalues) = total curvature.
    <em>Spectral entropy</em> (Shannon entropy of |&lambda;|/&Sigma;|&lambda;|) measures
    how spread out the spectrum is &mdash; high entropy means curvature is distributed
    across many directions, low entropy means it's concentrated in a few.
    <em>Max eigenvalue</em> tracks the sharpest direction.
    <em>Negative eigenvalue count</em> indicates saddle-point structure.
  </figcaption>
</figure>


<h2 id="expected">4. What We Expected</h2>

<p>
Under the spectral circuit hypothesis, we expected to see:
</p>

<ul>
  <li><strong>Band structure.</strong>  The eigenvalues should cluster into a small
  number of well-separated groups, each corresponding to a circuit.
  For MNIST digit recognition, plausible circuits include edge detection,
  stroke-width processing, and digit-specific template matching &mdash;
  perhaps 5&ndash;15 functional groups.</li>

  <li><strong>Staircase <span class="katex-like">n</span>(<span class="katex-like">&epsilon;</span>).</strong>
  The cluster count should drop in discrete steps at each spectral gap,
  not decay smoothly.</li>

  <li><strong>Progressive crystallization.</strong>  At initialization,
  the spectrum should be unstructured.  As training progresses,
  gaps should appear and widen &mdash; circuits separating out of the
  initial isotropic soup.</li>

  <li><strong>Dendrogram richness.</strong>  Multiple major branches of
  comparable height, reflecting a multi-level hierarchy of circuits
  and sub-circuits.</li>
</ul>

<p>
For the grokking experiment (modular addition), the prediction was even
more dramatic: the spectral structure should <em>reorganize</em> at the
grokking transition, as the network replaces a memorization solution
(~10K lookup-table-like parameters) with a generalization solution
(Fourier circuits computing modular arithmetic with far fewer effective
parameters).
</p>


<h2 id="observed">5. What We Actually See</h2>

<div class="callout callout-neg">
  <strong>The spectrum is not banded.</strong>
  The MNIST Hessian at all training stages shows a continuous spectrum,
  not discrete bands.  ~85% of eigenvalues cluster within
  [&minus;0.02, 0.02] (a near-zero bulk), ~1% are large outliers, and
  the rest fill a smooth continuum between.  The dominant spectral gap
  separates a single extreme eigenvalue from everything else &mdash; not
  the multiple gaps between circuit-like groups we predicted.
</div>

<p>
Specifically, at the final MNIST checkpoint (step 14070, 98.6% test accuracy):
</p>

<ul>
  <li>5,994 eigenvalues total, range [&minus;0.56, 52.1]</li>
  <li>5,075 eigenvalues (85%) have |&lambda;| &lt; 0.01</li>
  <li>Only 60 eigenvalues (1%) exceed 0.19</li>
  <li>The largest gap (39.8) separates a single eigenvalue from the rest</li>
  <li>The 2nd-largest gap (4.2) is 10&times; smaller</li>
  <li>All other gaps are smoothly distributed</li>
</ul>

<p>
The dendrogram reflects this: one or two dominant splits, then a
smooth cascade &mdash; not a rich hierarchy.  The
<span class="katex-like">n</span>(<span class="katex-like">&epsilon;</span>)
curves are smooth, not staircase-like.
</p>

<p>
The <em>evolution</em> over training does show something interesting:
progressive concentration.  The spectrum starts broadly distributed
and gradually sharpens as the near-zero bulk grows and outliers separate.
But this is generic optimization behavior (approaching a minimum flattens
most directions), not evidence of circuit-specific structure.
</p>


<h2 id="discussion">6. Why the Mismatch?</h2>

<p>
The absence of band structure doesn't necessarily mean circuits don't exist.
It means that <strong>if circuits exist, they don't live in orthogonal
Hessian eigenspaces</strong>.  Several explanations are possible:
</p>

<h3>The Hessian mixes circuits</h3>
<p>
The Hessian captures curvature of the <em>total loss</em>.  If two circuits
share any parameters (or even influence each other indirectly through the
loss function), their curvature contributions couple.  The resulting
eigenvalues don't neatly separate by circuit &mdash; they form hybrid
directions that blend multiple circuits.  This is analogous to how
molecular vibration spectra in crystals don't neatly separate by atom
type when there are strong inter-atomic couplings.
</p>

<h3>Circuits live in representation space, not parameter space</h3>
<p>
Mechanistic interpretability typically identifies circuits in terms of
<em>representations</em> (activations, attention patterns, features).  But
the Hessian lives in <em>parameter space</em>.  The mapping between the two is
highly nonlinear: many different parameter configurations can produce the
same representation.  Circuits that are cleanly separable in representation
space may be thoroughly entangled in parameter space.
</p>

<h3>We may need a different metric</h3>
<p>
The Hessian eigenspectrum is one view of loss landscape geometry, but
not the only one.  Alternatives that might reveal circuit structure better:
</p>

<ul>
  <li><strong>Fisher information matrix</strong> &mdash; captures sensitivity of the
  model's output distribution to parameter changes, which may align better
  with functional circuits.</li>
  <li><strong>Gradient covariance</strong> (across data points) &mdash; directions where
  different training examples push the model differently might correspond
  to task-specific circuits.</li>
  <li><strong>Neural tangent kernel</strong> &mdash; captures how parameter changes
  affect predictions, connecting parameter space to function space.</li>
  <li><strong>Block-diagonal Hessian</strong> (per layer or per module) &mdash; instead
  of the full Hessian, compute curvature within architectural blocks that
  correspond to hypothesized circuits.</li>
</ul>

<h3>The null hypothesis: it's just optimization geometry</h3>
<p>
The observed spectrum &mdash; a bulk of near-zero eigenvalues plus a few
outliers &mdash; is exactly what the Hessian literature predicts for
<em>any</em> overparameterized network near a minimum (Sagun et al. 2017,
Ghorbani et al. 2019).  The flat directions correspond to the parameter
redundancy inherent in overparameterization, and the outliers correspond
to a low-dimensional subspace that matters for generalization.  This
structure may be a universal feature of optimization geometry, largely
independent of what circuits the network implements.
</p>


<h2 id="experiments">7. Experiments</h2>

<h3>LeNet-tiny on MNIST <span class="experiment-badge badge-complete">Complete</span></h3>
<p>
A minimal conv-net (~6K parameters) trained on MNIST with SGD.  Full
Hessian eigendecomposition (all 5,994 eigenvalues + top/bottom 50
eigenvectors) at 25 checkpoints over 30 epochs.  Final test accuracy: 98.6%.
</p>
<p>
This is the experiment shown in the plots above.
<a href="mnist-notebook.html">Interactive marimo notebook &rarr;</a>
</p>

<h3>Modular Addition Grokking <span class="experiment-badge badge-wip">In Progress</span></h3>
<p>
A 1-layer transformer (d_model=128, 4 attention heads, ~95K parameters)
learning <code>a + b mod 113</code> &mdash; the canonical grokking task where
models suddenly generalize long after memorizing the training set.
Full-batch AdamW training (lr=10<sup>&minus;3</sup>, weight_decay=1.0),
50K steps, 30% train split.
</p>

<div class="callout">
  <strong>Status:</strong> Training completed (50K steps, 34 checkpoints saved).
  The model memorized perfectly (100% train accuracy by step 3K) but
  <strong>has not yet grokked</strong> &mdash; test accuracy remains at ~0.7% (chance).
  Grokking can take >100K steps; this run may need to be extended.
  Hessian computation via Lanczos iteration is partially complete (4/34
  checkpoints).  The Lanczos smallest-eigenvalue computation
  (<code>eigsh(which='SA')</code>) scales poorly on post-memorization Hessians
  and needs algorithmic improvement.
</div>

<p>
For this experiment, the full Hessian would be a 95K &times; 95K matrix (~72 GB).
Instead, we use Lanczos iteration with Hessian-vector products to approximate
the top and bottom 200 eigenvalues.  This gives us the spectral extremes
without forming the full matrix, at a cost of ~13 minutes per checkpoint
for early training stages.
</p>


<h2 id="next">8. Next Steps</h2>

<ul>
  <li>Extend modular addition training beyond 50K steps to observe grokking</li>
  <li>Fix Lanczos convergence for smallest eigenvalues (shift-invert mode,
  or looser tolerance)</li>
  <li>Try alternative metrics: Fisher information matrix, block-diagonal
  Hessian by layer, gradient covariance</li>
  <li>Test on models where circuits are mechanistically verified (e.g.,
  the Nanda et al. grokking analysis identifies specific Fourier circuits)
  to check whether those circuits correspond to spectral structure</li>
  <li>Explore relative gap metrics (gap / local eigenvalue density) that
  might reveal structure hidden in the near-zero bulk</li>
</ul>

<hr />

<p class="footer">
  <a href="https://github.com/CharlesR-W/hessian-dendrogram">Source on GitHub</a>
  &nbsp;&middot;&nbsp;
  <a href="mnist-notebook.html">Interactive MNIST Notebook</a>
  <br />
  Written with Claude
</p>

</body>
</html>
